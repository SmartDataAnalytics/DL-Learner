 /* 
 Note: DBpedia is always subject to change, solutions will change over time
 
 solutions:
   EXISTS http://dbpedia.org/property/monarch.TOP (length 3, depth 2)
 horizontal expansion: 1 to 3
 size of candidate set: 26
 properness tests (reasoner/short concept/too weak list): 101/0/0
 concept tests (reasoner/too weak list/overly general list/redundant concepts): 95/0/6/0
 Algorithm terminated succesfully.
 number of instance checks: 722 (114 multiple)
 instance check reasoning time: 1s 551ms ( 2ms per instance check)
 subsumption hierarchy queries: 30
 (complex) subsumption checks: 101 (14 multiple)
 subsumption reasoning time: 107ms ( 1ms per subsumption check)
 overall reasoning time: 1s 658ms (97,394% of overall runtime)
 overall algorithm runtime: 1s 702ms

 */

refinement.useAllConstructor = false;
//refinement.useExistsConstructor = true;
refinement.useNegation = false;

// SPARQL options
sparql.recursionDepth = 1;
sparql.predefinedFilter = "YAGO";
sparql.predefinedEndpoint = "DBPEDIA";

import("http://dbpedia.openlinksw.com:8890/sparql","SPARQL");

sparql.instances={"http://dbpedia.org/resource/Tony_Blair"
,"http://dbpedia.org/resource/Margaret_Thatcher"
,"http://dbpedia.org/resource/John_Major"

,"http://dbpedia.org/resource/James_Callaghan"

,"http://dbpedia.org/resource/Gerhard_Schr%C3%B6der"
,"http://dbpedia.org/resource/Helmut_Kohl"
,"http://dbpedia.org/resource/Jacques_Chirac"

};

/** examples **/
+"http://dbpedia.org/resource/Tony_Blair"
+"http://dbpedia.org/resource/Margaret_Thatcher"
+"http://dbpedia.org/resource/John_Major"
+"http://dbpedia.org/resource/James_Callaghan"

-"http://dbpedia.org/resource/Gerhard_Schr%C3%B6der"
-"http://dbpedia.org/resource/Helmut_Kohl"
-"http://dbpedia.org/resource/Jacques_Chirac"
